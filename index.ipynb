{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Principal Component Analysis \n",
    "\n",
    "## Introduction\n",
    "PCA could be very counter intuitive to start with. In this lesson, we shall simply attempt to develop an intuition around PCA in terms of dimension reduction and visualization of reduced dimensions. We shall look at a few simple examples in 1D and 2D domains to help you get a clear understanding of what we mean by explained variances, rotation of axis, derived features (components) etc. With this understanding, as we move towards implementing PCA is a number of different contexts, it would be much easier for you to picture what is happening behind the scenes.\n",
    "\n",
    "## Objectives\n",
    "You will be able to:\n",
    "- Develop a clear intuition around how PCA reduces dimensions\n",
    "- Understand principal components, what do they represent and how many components do we need\n",
    "- Understand the motivation behind mandatory data normalization prior to PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Principal Component Analysis ?\n",
    "\n",
    "> PCA (principal component analysis) is a popular machine learning technique for engineering and extracting important features as __components__ from a large set of features available in a given dataset. It extracts low dimensional set of features (a __feature Subspace__) from a high dimensional data set with a focus on capturing as much information as possible which is calculated as __captured variance__. \n",
    "\n",
    "PCA is also a great visualization tool it allows us to bring a high dimensional dataset into 2 or 3 dimensional subspace which becomes much easier to visualize and interpret.\n",
    "\n",
    "### PCA Intuition\n",
    "Let’s understand it using a simple intuitive example. Suppose we have a 3 dimensional dataset with a features set [x1, x2, x3] as shown in the 3D scatter plot below.\n",
    "<img src=\"3d.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The dataset contains 1000 points, and contains two classes, shown as green and red points. \n",
    "- The plane created by data points is on a slant of 45 degrees relative to both the x1 and x2 axes.\n",
    "\n",
    "The purpose of PCA is to __compress__ or __squash__ this dataset into two dimensions so it becomes easier to visualize as a 2D scatter plot. We would, however, like to preserve as much __spread__ or __variation__ from the original data as possible. This makes it a data reduction problem, but with a focus on __maximising the variance__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing dimensions without PCA\n",
    "One (rather naive) way of reducing our data from 3D to 2D, without using PCA or any other analytical reduction technique, would be to pick any 2 combination of the 3 dimensions and plot. Let's pick x1 and x2 and see how it looks. \n",
    "\n",
    "<img src=\"2d1.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not so bad. The red and green separation looks very clear and there is a reasonable amount of spread in all axis. This projection of data is __skewed__ though as the variance along the dropped axis has not been captured at all. Other axes combinations: x1, x3 and x2, x3 would produce similar results showing different levels of this skewness due to dropped variance. So this approach will not give us a __good projection__. \n",
    "\n",
    "### So what is a good projection ?\n",
    "\n",
    "The best projection is when we are looking directly at the plane i.e. __orthogonal__ to the plane, from an angle perpendicular to the plane, as shown below.\n",
    "<img src=\"2d2.png\" width=400>\n",
    "\n",
    "*[Visit here](https://en.wikipedia.org/wiki/Orthogonality) to have a quick read on orthogonality. It refers to \"perpendicularity\" between two vectors.*\n",
    "\n",
    "\n",
    "So the loss of variance by dropping features is not ideal. PCA allows to do a much better job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Note that PCA does not directly select important features and drops less important ones. PCA constructs new features from the existing that capture the most variation in the data.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Dimensions the PCA way\n",
    "\n",
    "One intuitive way you can think of PCA is that it __rotates__ the plane so that it __aligns__ to one of the 3 possible axis combinations. For example, if we could somehow do the following: \n",
    "1. __Rotate the plane__ so that it lies perfectly flat along the $(x1,x2)$ space \n",
    "2. After rotation, ignore x3 and the variance is still maintained due to rotation\n",
    "\n",
    "### A simpler Example\n",
    "\n",
    "This can be a bit counter intuitive. Let's think of an even simpler example we'll see how we can reduce a dataset from 2 to just 1 dimension. \n",
    "<img src=\"ex1.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, on the left we have 4 data points shown in red, in two demensional $(x_{1},x_{2})$ space. The purple line represents the 1-dimensional __subspace__ onto which PCA aims to project this data. \n",
    "\n",
    "The middle figure is where it all happens. Here, data points in red are being projected on the purple line as new points shown in green. \n",
    "\n",
    "PCA picks the 1D purple line while ensuring following:\n",
    "\n",
    "1. __Maximize__ the variance of the projected green points\n",
    "\n",
    "2. __Minimize__ the square of red-green distance (i.e. euclidiean squared distance of blue lines), using following formula: $$||x_{n}-\\tilde{x_{n}}||^2$$\n",
    "\n",
    "As a last step, the image on the right shows the original 2D data transformed from $(x_{1},x_{2})$ onto 1D, $u_{1}$ space. This new feature subspace is called __First Principal Component__. \n",
    "\n",
    "\n",
    "This intuition of PCA projecting 2D data onto a 1D line with __rotation__ can also generalize similarly to projecting 3D data onto a 2D plane shown below.\n",
    "\n",
    "<img src=\"3d22d.png\" width=800>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above figure, PCA requires two principal components $(u_{1},u_{2})$ to project 3D data from $(R,G,B)$ space to 2D, $(u_{1},u_{2})$ space. \n",
    "\n",
    "### What are Principal Components\n",
    "\n",
    "A principal component is a normalized linear combination of the original predictors in a data set. In image above, u1 and u2 are the principal components (otherwise, referred to as PC1 and PC2). IF we have a dataset containing features $X_1, X_2...,X_n$\n",
    "\n",
    "The principal component can be written as:\n",
    "\n",
    "$$u_1 = Φ_{11}X_1 + Φ_{21}X_2 + Φ_{31}X_3 + .... +Φ_{n1}X_n$$\n",
    "\n",
    "- $u_1$ is first principal component\n",
    "- $Φ$ (pronounced \"phi\") is called the __Loading Vector__ . It contains set of loadings ($Φ_1, Φ_2..$) of first principal component. It results in a line in n dimensional space which is closest to examples in the dataset. Closeness is measured using average squared euclidean distance as shown earlier.\n",
    "- $X_1..X_n$ are set of standardized features. \n",
    "\n",
    "\n",
    "The __First Principal Component__ ($u_{1}$ above) is found in the direction of highest variability and is a linear combination of the original features that captures the most variation in the data.Larger the variability captured in first component, larger the information captured by component. No other component can have variability higher than first principal component. The first principal component results in a line which is closest to the data i.e. it minimizes the sum of squared distance between a data point and the line.\n",
    "\n",
    "The __Second Principal Component__ ($u_{2}$ above) is __uncorrealted__ with first principal compoenent $u_{1}$ (that is $u_{1}$ is \"ORTHOGONAL\" to $u_{2}$). It is also a linear combination of the original features that captures the second most variation in the data. It can be computed as:\n",
    "\n",
    "$$u_2 = Φ_{12}X_1 + Φ_{22}X_2 + Φ_{32}X_3 + .... +Φ_{n2}X_n$$\n",
    "\n",
    "*Notice the direction of the components u1 and u2 sbove, as expected they are orthogonal. This suggests the correlation b/w these components in zero.*\n",
    "\n",
    "\n",
    "### How many principal components can we have ?\n",
    "\n",
    ">The number of possible principal components is equal to the original dimension of the data. In order to reduce the dimensionality of a dataset, we drop the principal components that explain the lowest amounts of variation in the data. \n",
    "\n",
    "For example to covert from 3D to 2D in the above plot, the third and final principal component ($u_{3}$ - not shown in the plot) is dropped.\n",
    "\n",
    "<img src=\"scree.png\", width=400>\n",
    "\n",
    "The plot above shows that in a 44 dimensional dataset,  30 components explain around 98.4% variance in the data set. In order words, using PCA can reduce 44 features to 30 without compromising on explained variance too much. This is the power of PCA. \n",
    "\n",
    "### Why is feature normalization necessary ?\n",
    "\n",
    "The principal components are calculated using normalized version of original features as the they may have different scales. For example: Imagine a data set with variables’ measuring units as gallons, kilometers, light years etc. Some features my show a large variance due to the nature of their scale.  Performing PCA on un-normalized variables will lead to insanely large loadings for variables with high variance. In turn, this will lead to dependence of a principal component on the variable with high variance as shown below:\n",
    "<img src=\"scale.png\" width=700>\n",
    "\n",
    "The image above shows results of PCA  on a 40 dimensional dataset twice (with unscaled and scaled features). You can see, first principal component is dominated by a variable Item_MRP. And, second principal component is dominated by a variable Item_Weight. This domination prevails due to high value of variance associated with a variable. When the variables are scaled, we get a much better representation of variables in 2D space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the original example \n",
    "As compared to simply dropping a dimension, applying PCA to the data shown in original example would results a reduced data which can be plotted in 2D as shown below:\n",
    "\n",
    "<img src=\"pcafinal.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, the plane now looks like a proper square and the centre circle looks like a circle. PCA finds the maximum variance for each principal component. There might be some loss of information in the process but overall we have the desired orthogonal view of the data. \n",
    "\n",
    "Next we shall look into steps necessary to perform PCA to successfully reduce the dimensions of a datset in a computational environment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Interactive PCA visualization demo](http://setosa.io/ev/principal-component-analysis/) - Highly Recommended\n",
    "- [An intuitive visual exmaple of PCA](https://algobeans.com/2016/06/15/principal-component-analysis-tutorial/)\n",
    "- [A One-Stop Shop for Principal Component Analysis](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c)\n",
    "- [A Tutorial on PCA](https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf) - For a deep dive into the maths behind PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "In this lesson we developed an intiution behind dimensionality reduction with PCA. We looked at a few very simple examples to see how PCA creates new dimensions (features) called principal components that attempt to capture the maximum variance in a given dataset. We looked at why normalization prior to PCA is standard approach. Next we shall look at the steps necessary to run a PCA experiment and achieve desired results. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
